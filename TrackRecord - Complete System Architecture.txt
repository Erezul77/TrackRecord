TrackRecord - Complete Technical & System Architecture
TABLE OF CONTENTS
System Overview
High-Level Architecture
Data Pipeline Architecture
Core Services Deep Dive
Database Architecture
API Architecture
Frontend Architecture
Infrastructure & DevOps
Security & Privacy
Scalability & Performance
Monitoring & Observability
Development Workflow
1. SYSTEM OVERVIEW
1.1 Core System Responsibilities
TrackRecord is a data processing and analytics platform that:

Captures predictions from multiple sources (Twitter, podcasts, articles)
Structures raw text into queryable prediction objects
Matches predictions to Polymarket markets
Simulates paper trading positions based on predictions
Tracks real money positions via Polymarket wallet integration
Calculates performance metrics (P&L, win rate, Sharpe, etc.)
Presents data via web interface and API
Alerts users to new predictions and resolutions
1.2 System Characteristics
Scale (Year 1):

100 pundits tracked
~50 predictions/day = 18k/year
~5 new market matches/hour
~1M page views/month
~100k API requests/day
~10k real-time WebSocket connections (future)
Performance Requirements:

Prediction capture latency: <5 minutes from publication
Market matching: <1 minute per prediction
P&L calculation: Real-time (sub-second)
Page load: <2 seconds (p95)
API response: <200ms (p95)
Reliability:

99.5% uptime (43 hours downtime/year acceptable)
Zero data loss (predictions are immutable)
Graceful degradation (show cached data if services fail)
2. HIGH-LEVEL ARCHITECTURE
2.1 System Diagram
┌─────────────────────────────────────────────────────────────────────┐
│                          DATA SOURCES                                │
│  Twitter API │ YouTube │ Podcasts │ News Sites │ TV Transcripts     │
└────────┬────────────────────────────────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     INGESTION LAYER                                  │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐             │
│  │   Twitter    │  │   Podcast    │  │   Web        │             │
│  │   Scraper    │  │   Parser     │  │   Scraper    │             │
│  └──────┬───────┘  └──────┬───────┘  └──────┬───────┘             │
│         │                  │                  │                      │
│         └──────────────────┴──────────────────┘                     │
│                            │                                         │
│                   ┌────────▼────────┐                               │
│                   │  Raw Content    │                               │
│                   │  Message Queue  │                               │
│                   │    (Redis)      │                               │
│                   └────────┬────────┘                               │
└────────────────────────────┼────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     PROCESSING LAYER                                 │
│  ┌──────────────────────────────────────────────────────────────┐  │
│  │              NLP Prediction Extraction Service               │  │
│  │  (Claude API / GPT-4) - Identifies predictive statements     │  │
│  └──────────────────────────┬───────────────────────────────────┘  │
│                             │                                        │
│                   ┌─────────▼─────────┐                             │
│                   │   Structured      │                             │
│                   │   Predictions     │                             │
│                   │   (PostgreSQL)    │                             │
│                   └─────────┬─────────┘                             │
│                             │                                        │
│  ┌──────────────────────────▼───────────────────────────────────┐  │
│  │           Market Matching Service                            │  │
│  │  (Embeddings + Semantic Search) - Match to Polymarket       │  │
│  └──────────────────────────┬───────────────────────────────────┘  │
│                             │                                        │
│                   ┌─────────▼─────────┐                             │
│                   │   Positions DB    │                             │
│                   │   (PostgreSQL)    │                             │
│                   └─────────┬─────────┘                             │
└────────────────────────────┼────────────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     ANALYTICS LAYER                                  │
│  ┌──────────────────┐  ┌──────────────────┐  ┌─────────────────┐  │
│  │  Position        │  │  Scoring         │  │  Real Money     │  │
│  │  Simulator       │  │  Engine          │  │  Tracker        │  │
│  │  (Paper Trading) │  │  (P&L, Sharpe)   │  │  (Polymarket)   │  │
│  └────────┬─────────┘  └────────┬─────────┘  └────────┬────────┘  │
│           │                     │                      │            │
│           └─────────────────────┴──────────────────────┘            │
│                                 │                                    │
│                       ┌─────────▼─────────┐                         │
│                       │   Analytics DB    │                         │
│                       │   (PostgreSQL)    │                         │
│                       │   + Redis Cache   │                         │
│                       └─────────┬─────────┘                         │
└────────────────────────────────┼────────────────────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     APPLICATION LAYER                                │
│  ┌──────────────────┐  ┌──────────────────┐  ┌─────────────────┐  │
│  │   Web API        │  │   GraphQL API    │  │   WebSocket     │  │
│  │   (FastAPI)      │  │   (Optional)     │  │   (Real-time)   │  │
│  └────────┬─────────┘  └────────┬─────────┘  └────────┬────────┘  │
└───────────┼─────────────────────┼─────────────────────┼────────────┘
            │                     │                     │
            └─────────────────────┴─────────────────────┘
                                 │
                                 ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     PRESENTATION LAYER                               │
│  ┌──────────────────┐  ┌──────────────────┐  ┌─────────────────┐  │
│  │   Web App        │  │   Browser        │  │   Mobile App    │  │
│  │   (Next.js)      │  │   Extension      │  │   (Future)      │  │
│  └──────────────────┘  └──────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│                     EXTERNAL INTEGRATIONS                            │
│  ┌──────────────────┐  ┌──────────────────┐  ┌─────────────────┐  │
│  │   Polymarket     │  │   Twitter/X      │  │   LLM APIs      │  │
│  │   API            │  │   API            │  │   (Claude/GPT)  │  │
│  └──────────────────┘  └──────────────────┘  └─────────────────┘  │
└─────────────────────────────────────────────────────────────────────┘
2.2 Architecture Principles
1. Separation of Concerns

Each layer has single responsibility
Services communicate via well-defined interfaces
Easy to swap components (e.g., change LLM provider)
2. Event-Driven

Async processing via message queues
Decouple data capture from processing
Enable replay/reprocessing of events
3. Immutability

Predictions never modified after capture
Append-only event log
Audit trail for all changes
4. Cache-First

Aggressive caching at every layer
Stale data acceptable for most views
Real-time only where necessary
5. Fail Gracefully

If NLP fails, queue for manual review
If Polymarket API down, use cached data
Never show blank page to users
3. DATA PIPELINE ARCHITECTURE
3.1 Ingestion Pipeline
python
# Conceptual flow, not production code

class IngestionPipeline:
    """
    Orchestrates data capture from multiple sources
    """
    
    def __init__(self):
        self.sources = [
            TwitterSource(),
            PodcastSource(),
            NewsSource(),
            YouTubeSource()
        ]
        self.queue = RedisQueue('raw_content')
    
    async def run(self):
        """
        Continuously polls sources and enqueues new content
        """
        while True:
            for source in self.sources:
                try:
                    items = await source.fetch_new_content()
                    for item in items:
                        await self.queue.enqueue({
                            'source': source.name,
                            'content': item.text,
                            'author': item.author,
                            'url': item.url,
                            'timestamp': item.timestamp,
                            'metadata': item.metadata
                        })
                except Exception as e:
                    log_error(f"Source {source.name} failed", e)
                    # Continue with other sources
            
            await asyncio.sleep(60)  # Poll every minute
```

### 3.2 Twitter/X Integration

**Architecture:**
```
Twitter API v2
    │
    ├─► User Timeline Endpoint (pull pundit tweets)
    ├─► Search Endpoint (find predictions via keywords)
    └─► Streaming API (real-time monitoring)
         │
         ▼
    Rate Limiter (450 requests/15min per endpoint)
         │
         ▼
    Tweet Parser
         │
         ├─► Filter: Only tweets by tracked pundits
         ├─► Filter: Must contain predictive language
         ├─► Extract: Full text, thread context, links
         └─► Deduplicate: Check if already processed
              │
              ▼
    Enqueue to Raw Content Queue
Implementation Details:

python
class TwitterSource:
    """
    Captures tweets from tracked pundits
    """
    
    def __init__(self, api_key, bearer_token):
        self.client = tweepy.Client(bearer_token=bearer_token)
        self.tracked_users = self.load_tracked_users()  # From DB
        self.rate_limiter = RateLimiter(450, 900)  # 450 per 15min
    
    async def fetch_new_content(self):
        """
        Fetch recent tweets from tracked users
        """
        items = []
        
        for user in self.tracked_users:
            await self.rate_limiter.wait()
            
            try:
                tweets = self.client.get_users_tweets(
                    user.twitter_id,
                    max_results=10,
                    since_id=user.last_tweet_id,
                    tweet_fields=['created_at', 'conversation_id'],
                    expansions=['referenced_tweets.id']
                )
                
                for tweet in tweets.data or []:
                    # Check if part of thread
                    thread = self.get_thread_context(tweet)
                    
                    items.append(RawContent(
                        text=thread,  # Full thread text
                        author=user.name,
                        url=f"https://twitter.com/{user.username}/status/{tweet.id}",
                        timestamp=tweet.created_at,
                        metadata={
                            'tweet_id': tweet.id,
                            'thread': True if len(thread) > tweet.text else False
                        }
                    ))
                
                # Update last seen tweet
                if tweets.data:
                    user.last_tweet_id = tweets.data[0].id
                    await self.db.update_user(user)
                    
            except tweepy.TooManyRequests:
                logger.warning(f"Rate limited on user {user.name}")
                break  # Stop and try again next cycle
            except Exception as e:
                logger.error(f"Failed to fetch tweets for {user.name}", e)
        
        return items
    
    def get_thread_context(self, tweet):
        """
        If tweet is part of thread, fetch full thread
        """
        if not tweet.conversation_id:
            return tweet.text
        
        # Fetch all tweets in conversation
        thread_tweets = self.client.search_recent_tweets(
            query=f"conversation_id:{tweet.conversation_id}",
            max_results=100
        )
        
        # Sort by timestamp and concatenate
        sorted_tweets = sorted(thread_tweets.data, key=lambda t: t.created_at)
        return "\n\n".join([t.text for t in sorted_tweets])
```

**Challenges & Solutions:**

| Challenge | Solution |
|-----------|----------|
| Rate limits | Intelligent backoff, prioritize high-value pundits |
| Thread detection | Use conversation_id to fetch full context |
| Quote tweets | Expand referenced tweets to include context |
| Deleted tweets | Cache immediately on capture, mark as deleted later |
| Private accounts | Manual monitoring or skip |

### 3.3 Podcast/Video Integration

**Architecture:**
```
Podcast RSS Feeds / YouTube Channels
    │
    ▼
Download Audio/Video
    │
    ▼
Whisper API (OpenAI) - Speech to Text
    │
    ▼
Transcript with Timestamps
    │
    ▼
Segment by Speaker (diarization)
    │
    ▼
Filter: Only tracked pundit's segments
    │
    ▼
Enqueue to Raw Content Queue
Implementation:

python
class PodcastSource:
    """
    Transcribes podcasts and extracts predictions
    """
    
    def __init__(self):
        self.feeds = self.load_podcast_feeds()  # RSS URLs
        self.whisper = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.storage = S3Client()
    
    async def fetch_new_content(self):
        items = []
        
        for feed in self.feeds:
            # Parse RSS feed
            episodes = await self.parse_feed(feed.url)
            
            for episode in episodes:
                if await self.already_processed(episode.guid):
                    continue
                
                # Download audio
                audio_path = await self.download_audio(episode.audio_url)
                
                # Transcribe
                transcript = await self.transcribe(audio_path)
                
                # Identify tracked pundits in transcript
                for pundit in self.identify_pundits(transcript, episode):
                    items.append(RawContent(
                        text=pundit.segments,  # Their speaking segments
                        author=pundit.name,
                        url=episode.url,
                        timestamp=episode.published_date,
                        metadata={
                            'episode_title': episode.title,
                            'podcast': feed.name,
                            'duration': episode.duration
                        }
                    ))
                
                # Mark as processed
                await self.mark_processed(episode.guid)
        
        return items
    
    async def transcribe(self, audio_path):
        """
        Use Whisper API for transcription
        """
        with open(audio_path, 'rb') as audio:
            response = await self.whisper.audio.transcriptions.create(
                model="whisper-1",
                file=audio,
                response_format="verbose_json",
                timestamp_granularities=["segment"]
            )
        
        # Returns: [{text, start, end, speaker}, ...]
        return response.segments
    
    def identify_pundits(self, transcript, episode):
        """
        Use speaker diarization + name matching to find tracked pundits
        """
        # Look for names in episode description
        guests = self.extract_guest_names(episode.description)
        
        # Match against tracked pundits
        tracked = [g for g in guests if g in self.tracked_pundits]
        
        # Group segments by speaker and assign to pundit
        # (This is heuristic - may need manual verification)
        return self.assign_segments_to_pundits(transcript, tracked)
```

**Cost Management:**

- Whisper API: ~$0.006/minute
- Average podcast: 60 minutes = $0.36
- 10 podcasts/day = $3.60/day = $108/month
- **Optimization:** Only transcribe episodes with tracked guests

### 3.4 News/Article Integration

**Architecture:**
```
RSS Feeds / Google News API / Manual URLs
    │
    ▼
Web Scraper (Playwright/Scrapy)
    │
    ▼
HTML → Clean Text (Trafilatura)
    │
    ▼
Author Detection (byline, metadata)
    │
    ▼
Filter: Only tracked pundit articles
    │
    ▼
Enqueue to Raw Content Queue
Implementation:

python
class NewsSource:
    """
    Scrapes news sites and opinion pieces
    """
    
    def __init__(self):
        self.feeds = self.load_news_feeds()
        self.scraper = PlaywrightScraper()
    
    async def fetch_new_content(self):
        items = []
        
        for feed in self.feeds:
            articles = await self.fetch_feed(feed.url)
            
            for article in articles:
                # Check if article is by tracked pundit
                author = await self.detect_author(article)
                
                if author and author in self.tracked_pundits:
                    # Scrape full article
                    content = await self.scraper.scrape(article.url)
                    
                    items.append(RawContent(
                        text=content.text,
                        author=author.name,
                        url=article.url,
                        timestamp=article.published_date,
                        metadata={
                            'publication': feed.name,
                            'headline': article.title
                        }
                    ))
        
        return items
    
    async def detect_author(self, article):
        """
        Extract author from article metadata/byline
        """
        # Try structured data
        if article.author:
            return self.match_pundit(article.author)
        
        # Scrape and look for byline
        html = await self.scraper.fetch(article.url)
        author = self.extract_byline(html)
        
        return self.match_pundit(author)
```

---

## 4. CORE SERVICES DEEP DIVE

### 4.1 NLP Prediction Extraction Service

**Purpose:** Transform raw text into structured prediction objects

**Architecture:**
```
Raw Content (from queue)
    │
    ▼
Preprocessing
    ├─► Clean text (remove URLs, mentions)
    ├─► Chunk if too long (>4k tokens)
    └─► Add context (author, date, source)
        │
        ▼
LLM Analysis (Claude/GPT-4)
    ├─► Prompt: "Extract all predictions from this text"
    ├─► Output: Structured JSON
    └─► Parse response
        │
        ▼
Validation
    ├─► Check required fields present
    ├─► Validate timeframe is future
    ├─► Confidence level mappable
    └─► Claim is specific enough
        │
        ▼
Store Prediction
    ├─► Generate SHA-256 hash (immutability)
    ├─► Insert into PostgreSQL
    └─► Emit "prediction.created" event
        │
        ▼
Enqueue for Market Matching
LLM Prompt Engineering:

python
EXTRACTION_PROMPT = """
You are analyzing text to extract specific predictions about future events.

Text to analyze:
---
{text}
---

Author: {author}
Date published: {date}
Source: {source_type}

Extract ALL predictions about future events. A prediction must:
1. Make a claim about something that will/won't happen in the future
2. Be specific enough to verify (not vague like "things will get better")
3. Have a timeframe (explicit or implicit)

For each prediction, provide:
- claim: The specific thing being predicted (as a yes/no question)
- confidence: The author's implied confidence (certain/high/medium/low/speculative)
- timeframe: When this will be verifiable (date, month, year, or "within X months")
- quote: Exact quote from text (verbatim, including context)
- conditionality: Any conditions ("if X then Y") or null
- category: politics/economy/markets/crypto/tech/other

Return ONLY valid JSON array:
[
  {
    "claim": "Donald Trump will win the 2024 presidential election",
    "confidence": "high",
    "timeframe": "2024-11-05",
    "quote": "Trump is going to win, mark my words. The polling is clear.",
    "conditionality": null,
    "category": "politics"
  },
  ...
]

If no predictions found, return empty array: []

Do NOT include:
- Descriptions of past events
- Vague opinions without specific claims
- Questions posed by author
- Hypotheticals not presented as predictions
"""

class PredictionExtractor:
    """
    Extracts predictions from raw content using LLM
    """
    
    def __init__(self):
        self.llm = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
        self.model = "claude-sonnet-4-20250514"
    
    async def extract(self, raw_content: RawContent) -> List[Prediction]:
        """
        Extract structured predictions from raw content
        """
        # Prepare prompt
        prompt = EXTRACTION_PROMPT.format(
            text=raw_content.text,
            author=raw_content.author,
            date=raw_content.timestamp.isoformat(),
            source_type=raw_content.metadata.get('source_type')
        )
        
        # Call LLM
        try:
            response = await self.llm.messages.create(
                model=self.model,
                max_tokens=4000,
                temperature=0,  # Deterministic
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            # Parse JSON response
            predictions_json = json.loads(response.content[0].text)
            
        except json.JSONDecodeError:
            logger.error(f"LLM returned invalid JSON for content {raw_content.id}")
            # Queue for manual review
            await self.queue_for_manual_review(raw_content)
            return []
        except Exception as e:
            logger.error(f"LLM extraction failed: {e}")
            return []
        
        # Convert to Prediction objects
        predictions = []
        for pred_data in predictions_json:
            try:
                prediction = Prediction(
                    id=generate_uuid(),
                    pundit_id=self.get_pundit_id(raw_content.author),
                    claim=pred_data['claim'],
                    confidence=self.map_confidence(pred_data['confidence']),
                    timeframe=self.parse_timeframe(pred_data['timeframe']),
                    quote=pred_data['quote'],
                    conditionality=pred_data.get('conditionality'),
                    category=pred_data['category'],
                    source_url=raw_content.url,
                    captured_at=datetime.utcnow(),
                    content_hash=self.hash_content(raw_content),
                    status='pending_match'
                )
                
                # Validate
                if self.validate(prediction):
                    predictions.append(prediction)
                else:
                    logger.warning(f"Invalid prediction extracted: {prediction}")
                    
            except Exception as e:
                logger.error(f"Failed to create prediction from {pred_data}: {e}")
        
        return predictions
    
    def map_confidence(self, confidence_str: str) -> float:
        """
        Map verbal confidence to probability
        """
        mapping = {
            'certain': 0.95,
            'high': 0.80,
            'medium': 0.60,
            'low': 0.40,
            'speculative': 0.25
        }
        return mapping.get(confidence_str.lower(), 0.50)
    
    def parse_timeframe(self, timeframe_str: str) -> datetime:
        """
        Parse timeframe into resolution date
        """
        # Try parsing as date
        try:
            return datetime.fromisoformat(timeframe_str)
        except:
            pass
        
        # Try parsing relative ("within 6 months")
        if 'month' in timeframe_str:
            months = int(re.search(r'\d+', timeframe_str).group())
            return datetime.utcnow() + timedelta(days=30 * months)
        
        if 'year' in timeframe_str:
            years = int(re.search(r'\d+', timeframe_str).group())
            return datetime.utcnow() + timedelta(days=365 * years)
        
        # Default: 1 year out
        return datetime.utcnow() + timedelta(days=365)
    
    def validate(self, prediction: Prediction) -> bool:
        """
        Validate prediction meets minimum quality standards
        """
        # Must have claim
        if not prediction.claim or len(prediction.claim) < 10:
            return False
        
        # Timeframe must be in future
        if prediction.timeframe < datetime.utcnow():
            return False
        
        # Confidence must be valid
        if not 0 < prediction.confidence < 1:
            return False
        
        # Quote must be present
        if not prediction.quote:
            return False
        
        return True
    
    def hash_content(self, raw_content: RawContent) -> str:
        """
        Generate SHA-256 hash of raw content for immutability
        """
        content_str = f"{raw_content.author}|{raw_content.timestamp.isoformat()}|{raw_content.text}"
        return hashlib.sha256(content_str.encode()).hexdigest()
```

**Error Handling:**

- **LLM returns invalid JSON:** Queue for manual review, log error
- **LLM timeout:** Retry up to 3 times with exponential backoff
- **Rate limit:** Use token bucket algorithm, pause and resume
- **Cost control:** Track spend per author, set budget alerts

**Quality Assurance:**

- Manual review sample (10% of predictions)
- User flagging system ("this isn't a prediction")
- Pundit can contest/verify their predictions
- A/B test different prompts, measure accuracy

### 4.2 Market Matching Service

**Purpose:** Match predictions to Polymarket markets

**Architecture:**
```
Prediction (from database)
    │
    ▼
Generate Embedding (OpenAI ada-002)
    │
    ▼
Fetch Active Polymarket Markets
    │
    ▼
Compute Similarity (Cosine Distance)
    │
    ▼
Rank Matches (Top 5)
    │
    ├─► Confidence > 0.85 → Auto-match
    ├─► Confidence 0.70-0.85 → Manual review queue
    └─► Confidence < 0.70 → No match
        │
        ▼
Record Match
    ├─► Store in positions table
    ├─► Capture market price at match time
    └─► Emit "position.opened" event
Implementation:

python
class MarketMatcher:
    """
    Matches predictions to Polymarket markets using semantic similarity
    """
    
    def __init__(self):
        self.polymarket = PolymarketClient()
        self.embeddings = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.vector_db = PgVector()  # PostgreSQL with pgvector extension
    
    async def match(self, prediction: Prediction) -> Optional[Match]:
        """
        Find best matching Polymarket market for prediction
        """
        # Generate embedding for prediction claim
        pred_embedding = await self.get_embedding(prediction.claim)
        
        # Get active markets in same category
        markets = await self.polymarket.get_active_markets(
            category=prediction.category,
            end_date_after=prediction.timeframe
        )
        
        if not markets:
            logger.info(f"No active markets for prediction {prediction.id}")
            return None
        
        # Generate embeddings for markets (cache these!)
        market_embeddings = []
        for market in markets:
            embedding = await self.get_market_embedding(market)
            market_embeddings.append((market, embedding))
        
        # Compute cosine similarities
        similarities = []
        for market, emb in market_embeddings:
            sim = self.cosine_similarity(pred_embedding, emb)
            similarities.append((market, sim))
        
        # Sort by similarity
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # Get top match
        best_market, best_score = similarities[0]
        
        # Decide whether to auto-match
        if best_score > 0.85:
            status = 'auto_matched'
        elif best_score > 0.70:
            status = 'needs_review'
        else:
            status = 'no_match'
            return None
        
        # Create match record
        match = Match(
            prediction_id=prediction.id,
            market_id=best_market.id,
            market_question=best_market.question,
            market_slug=best_market.slug,
            similarity_score=best_score,
            match_type=status,
            matched_at=datetime.utcnow(),
            entry_price=best_market.current_price,
            alternatives=similarities[1:5]  # Store runner-ups
        )
        
        return match
    
    async def get_embedding(self, text: str) -> np.ndarray:
        """
        Generate embedding vector for text
        """
        response = await self.embeddings.embeddings.create(
            input=text,
            model="text-embedding-3-small"  # 1536 dimensions, cheap
        )
        return np.array(response.data[0].embedding)
    
    async def get_market_embedding(self, market: PolymarketMarket) -> np.ndarray:
        """
        Get or generate embedding for market question
        Cache embeddings since markets don't change
        """
        cache_key = f"market_embedding:{market.id}"
        cached = await self.redis.get(cache_key)
        
        if cached:
            return np.frombuffer(cached, dtype=np.float32)
        
        # Generate embedding
        embedding = await self.get_embedding(market.question)
        
        # Cache for 7 days
        await self.redis.setex(
            cache_key,
            604800,  # 7 days
            embedding.tobytes()
        )
        
        return embedding
    
    def cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
        """
        Compute cosine similarity between two vectors
        """
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
Polymarket API Integration:

python
class PolymarketClient:
    """
    Client for Polymarket API
    """
    
    BASE_URL = "https://clob.polymarket.com"
    
    def __init__(self):
        self.session = httpx.AsyncClient()
        self.cache = Redis()
    
    async def get_active_markets(self, category=None, end_date_after=None):
        """
        Fetch active markets from Polymarket
        """
        params = {
            'active': True,
            'closed': False
        }
        
        if category:
            params['tag'] = self.map_category(category)
        
        if end_date_after:
            params['end_date_min'] = end_date_after.isoformat()
        
        # Check cache first
        cache_key = f"markets:{json.dumps(params, sort_keys=True)}"
        cached = await self.cache.get(cache_key)
        
        if cached:
            return json.loads(cached)
        
        # Fetch from API
        response = await self.session.get(
            f"{self.BASE_URL}/markets",
            params=params
        )
        
        markets = response.json()
        
        # Cache for 5 minutes
        await self.cache.setex(cache_key, 300, json.dumps(markets))
        
        return markets
    
    async def get_market_price(self, market_id: str) -> float:
        """
        Get current price for a market
        """
        response = await self.session.get(
            f"{self.BASE_URL}/markets/{market_id}"
        )
        
        market = response.json()
        return market['outcome_prices'][0]  # YES price
    
    async def get_market_by_slug(self, slug: str):
        """
        Get market details by slug
        """
        response = await self.session.get(
            f"{self.BASE_URL}/markets",
            params={'slug': slug}
        )
        
        return response.json()[0]
    
    def map_category(self, internal_category: str) -> str:
        """
        Map our categories to Polymarket tags
        """
        mapping = {
            'politics': 'politics',
            'economy': 'economics',
            'crypto': 'crypto',
            'tech': 'technology',
            'markets': 'finance'
        }
        return mapping.get(internal_category, 'misc')
Manual Review Queue:

For matches with 0.70-0.85 confidence, store in review queue:

sql
CREATE TABLE match_review_queue (
    id UUID PRIMARY KEY,
    prediction_id UUID REFERENCES predictions(id),
    suggested_market_id VARCHAR,
    similarity_score FLOAT,
    status VARCHAR DEFAULT 'pending',  -- pending/approved/rejected
    reviewed_by UUID,
    reviewed_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);
```

Admin dashboard allows reviewers to:
- View prediction and suggested market side-by-side
- Approve match (creates position)
- Reject match (mark prediction as unmatchable)
- Suggest alternative market

### 4.3 Position Simulator

**Purpose:** Calculate paper trading P&L for predictions

**Architecture:**
```
Position Created (from match)
    │
    ▼
Record Entry
    ├─► Market price at prediction time
    ├─► Calculate shares purchased ($100 / price)
    └─► Store in positions table
        │
        ▼
Periodic Price Updates (every hour)
    ├─► Fetch current market price
    ├─► Calculate unrealized P&L
    └─► Update cache
        │
        ▼
Market Resolves
    ├─► Fetch resolution (YES/NO)
    ├─► Calculate realized P&L
    ├─► Update position status to 'closed'
    └─► Recalculate pundit metrics
Implementation:

python
class PositionSimulator:
    """
    Simulates paper trading positions based on predictions
    """
    
    POSITION_SIZES = {
        0.95: 1000,  # "Certain"
        0.80: 500,   # "High confidence"
        0.60: 300,   # "Medium confidence"
        0.40: 100,   # "Low confidence"
        0.25: 50     # "Speculative"
    }
    
    def __init__(self):
        self.polymarket = PolymarketClient()
        self.db = Database()
    
    async def create_position(self, match: Match, prediction: Prediction):
        """
        Create paper trading position from matched prediction
        """
        # Determine position size based on confidence
        position_size = self.get_position_size(prediction.confidence)
        
        # Get market price at time of prediction
        entry_price = match.entry_price
        
        # Calculate shares purchased
        shares = position_size / entry_price
        
        # Create position record
        position = Position(
            id=generate_uuid(),
            prediction_id=prediction.id,
            pundit_id=prediction.pundit_id,
            market_id=match.market_id,
            market_question=match.market_question,
            entry_price=entry_price,
            entry_timestamp=prediction.captured_at,
            shares=shares,
            position_size=position_size,
            status='open',
            unrealized_pnl=0,
            realized_pnl=None,
            created_at=datetime.utcnow()
        )
        
        # Store in database
        await self.db.insert('positions', position)
        
        # Emit event
        await self.events.emit('position.opened', position)
        
        logger.info(f"Created position {position.id} for pundit {prediction.pundit_id}")
        
        return position
    
    def get_position_size(self, confidence: float) -> float:
        """
        Map confidence to position size
        """
        # Find closest confidence level
        closest = min(self.POSITION_SIZES.keys(), 
                     key=lambda x: abs(x - confidence))
        return self.POSITION_SIZES[closest]
    
    async def update_prices(self):
        """
        Update all open positions with current market prices
        Run this every hour
        """
        open_positions = await self.db.query(
            "SELECT * FROM positions WHERE status = 'open'"
        )
        
        for position in open_positions:
            try:
                # Fetch current market price
                current_price = await self.polymarket.get_market_price(
                    position.market_id
                )
                
                # Calculate unrealized P&L
                current_value = position.shares * current_price
                unrealized_pnl = current_value - position.position_size
                
                # Update position
                await self.db.update('positions', position.id, {
                    'current_price': current_price,
                    'unrealized_pnl': unrealized_pnl,
                    'last_updated': datetime.utcnow()
                })
                
                # Update cache (for fast reads)
                await self.cache_position(position.id, {
                    'current_price': current_price,
                    'unrealized_pnl': unrealized_pnl
                })
                
            except Exception as e:
                logger.error(f"Failed to update position {position.id}: {e}")
    
    async def resolve_position(self, position: Position, outcome: str):
        """
        Mark position as resolved and calculate final P&L
        
        Args:
            position: The position to resolve
            outcome: 'YES' or 'NO'
        """
        # Calculate final value
        if outcome == 'YES':
            exit_price = 1.00
        else:
            exit_price = 0.00
        
        final_value = position.shares * exit_price
        realized_pnl = final_value - position.position_size
        
        # Update position
        await self.db.update('positions', position.id, {
            'status': 'closed',
            'exit_price': exit_price,
            'exit_timestamp': datetime.utcnow(),
            'realized_pnl': realized_pnl,
            'outcome': outcome
        })
        
        # Update pundit metrics (trigger recalculation)
        await self.recalculate_pundit_metrics(position.pundit_id)
        
        # Emit event
        await self.events.emit('position.closed', position)
        
        logger.info(f"Resolved position {position.id}: {outcome}, P&L: {realized_pnl}")
Polymarket Resolution Monitoring:

python
class ResolutionMonitor:
    """
    Monitors Polymarket markets for resolutions
    """
    
    def __init__(self):
        self.polymarket = PolymarketClient()
        self.simulator = PositionSimulator()
    
    async def run(self):
        """
        Continuously check for market resolutions
        Run this every 10 minutes
        """
        while True:
            try:
                # Get all open positions
                open_positions = await self.db.query(
                    """
                    SELECT DISTINCT market_id, market_slug 
                    FROM positions 
                    WHERE status = 'open'
                    """
                )
                
                for market_id, market_slug in open_positions:
                    # Check if market has resolved
                    market = await self.polymarket.get_market_by_slug(market_slug)
                    
                    if market['closed'] and market['resolved']:
                        outcome = market['winning_outcome']
                        
                        # Resolve all positions for this market
                        positions = await self.db.query(
                            "SELECT * FROM positions WHERE market_id = ?",
                            (market_id,)
                        )
                        
                        for position in positions:
                            await self.simulator.resolve_position(
                                position,
                                outcome
                            )
                
                await asyncio.sleep(600)  # Check every 10 minutes
                
            except Exception as e:
                logger.error(f"Resolution monitor error: {e}")
                await asyncio.sleep(60)
```

### 4.4 Real Money Tracker

**Purpose:** Track verified pundits' actual Polymarket positions

**Architecture:**
```
Pundit Links Wallet (OAuth)
    │
    ▼
Store Wallet Address (encrypted)
    │
    ▼
Periodic Sync (every 15 minutes)
    ├─► Fetch on-chain positions via Polymarket API
    ├─► Match positions to predictions
    └─► Calculate real P&L
        │
        ▼
Display on Profile
    ├─► Separate "Real Money" tab
    └─► Badge: "Verified Predictor ✅"
Implementation:

python
class RealMoneyTracker:
    """
    Tracks verified pundits' real Polymarket positions
    """
    
    def __init__(self):
        self.polymarket = PolymarketClient()
        self.db = Database()
    
    async def link_wallet(self, pundit_id: str, wallet_address: str):
        """
        Link a pundit's Polymarket wallet for tracking
        """
        # Verify wallet ownership (sign message)
        is_valid = await self.verify_wallet_ownership(wallet_address)
        
        if not is_valid:
            raise ValueError("Wallet verification failed")
        
        # Store encrypted
        encrypted_address = self.encrypt(wallet_address)
        
        await self.db.update('pundits', pundit_id, {
            'wallet_address': encrypted_address,
            'verified': True,
            'verified_at': datetime.utcnow()
        })
        
        logger.info(f"Linked wallet for pundit {pundit_id}")
    
    async def sync_positions(self, pundit_id: str):
        """
        Sync pundit's on-chain positions from Polymarket
        """
        # Get wallet address
        pundit = await self.db.get('pundits', pundit_id)
        wallet_address = self.decrypt(pundit.wallet_address)
        
        # Fetch positions from Polymarket
        positions = await self.polymarket.get_user_positions(wallet_address)
        
        # Match positions to predictions
        for position in positions:
            # Try to find matching prediction
            prediction = await self.find_matching_prediction(
                pundit_id,
                position.market_id,
                position.created_at
            )
            
            if prediction:
                # Store real money position
                await self.db.insert('real_positions', {
                    'id': generate_uuid(),
                    'pundit_id': pundit_id,
                    'prediction_id': prediction.id,
                    'market_id': position.market_id,
                    'entry_price': position.avg_entry_price,
                    'shares': position.shares,
                    'current_value': position.current_value,
                    'realized_pnl': position.realized_pnl,
                    'status': position.status,
                    'synced_at': datetime.utcnow()
                })
    
    async def find_matching_prediction(self, pundit_id, market_id, position_date):
        """
        Find prediction that matches this real position
        """
        # Look for prediction made around same time (+/- 24 hours)
        # for same market
        prediction = await self.db.query_one(
            """
            SELECT p.* FROM predictions p
            JOIN matches m ON p.id = m.prediction_id
            WHERE p.pundit_id = ?
              AND m.market_id = ?
              AND p.captured_at BETWEEN ? AND ?
            ORDER BY ABS(EXTRACT(EPOCH FROM p.captured_at - ?))
            LIMIT 1
            """,
            (
                pundit_id,
                market_id,
                position_date - timedelta(hours=24),
                position_date + timedelta(hours=24),
                position_date
            )
        )
        
        return prediction
    
    async def verify_wallet_ownership(self, wallet_address: str) -> bool:
        """
        Verify user owns the wallet via signature
        """
        # Generate random nonce
        nonce = secrets.token_hex(32)
        
        # Store nonce temporarily
        await self.cache.setex(f"wallet_verify:{wallet_address}", 300, nonce)
        
        # Prompt user to sign message with their wallet
        # (Frontend handles this via MetaMask/WalletConnect)
        
        # This is placeholder - actual verification happens in frontend
        return True
Privacy & Security:

Wallet addresses encrypted at rest (AES-256)
Read-only access (never ask for private keys)
User can revoke access anytime
Position data cached, not stored permanently
Optional: Allow pundits to hide specific positions
5. DATABASE ARCHITECTURE
5.1 Schema Design
Core Tables:

sql
-- Pundits (people we track)
CREATE TABLE pundits (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    username VARCHAR(255),  -- Twitter handle
    twitter_id VARCHAR(50),
    affiliation VARCHAR(255),  -- NYT, CNBC, etc.
    bio TEXT,
    avatar_url VARCHAR(500),
    domains TEXT[],  -- {politics, economy, crypto}
    wallet_address BYTEA,  -- Encrypted, nullable
    verified BOOLEAN DEFAULT FALSE,
    verified_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    
    -- Tracking metadata
    last_tweet_id VARCHAR(50),
    last_checked_at TIMESTAMP,
    
    UNIQUE(twitter_id),
    UNIQUE(username)
);

CREATE INDEX idx_pundits_verified ON pundits(verified) WHERE verified = TRUE;
CREATE INDEX idx_pundits_domains ON pundits USING GIN(domains);

-- Raw content captured from sources
CREATE TABLE raw_content (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    pundit_id UUID REFERENCES pundits(id),
    source_type VARCHAR(50) NOT NULL,  -- twitter, podcast, article
    text TEXT NOT NULL,
    url VARCHAR(1000),
    published_at TIMESTAMP NOT NULL,
    captured_at TIMESTAMP DEFAULT NOW(),
    metadata JSONB,  -- Flexible storage for source-specific data
    processed BOOLEAN DEFAULT FALSE,
    processed_at TIMESTAMP,
    
    content_hash VARCHAR(64) UNIQUE NOT NULL  -- SHA-256 for deduplication
);

CREATE INDEX idx_raw_content_pundit ON raw_content(pundit_id);
CREATE INDEX idx_raw_content_processed ON raw_content(processed) WHERE processed = FALSE;
CREATE INDEX idx_raw_content_hash ON raw_content(content_hash);

-- Extracted predictions
CREATE TABLE predictions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    pundit_id UUID REFERENCES pundits(id) NOT NULL,
    raw_content_id UUID REFERENCES raw_content(id),
    
    -- Prediction content
    claim TEXT NOT NULL,  -- "Donald Trump will win Pennsylvania"
    confidence FLOAT NOT NULL CHECK (confidence > 0 AND confidence < 1),
    timeframe TIMESTAMP NOT NULL,  -- When this can be verified
    quote TEXT NOT NULL,  -- Exact quote from source
    conditionality TEXT,  -- "If Biden drops out, then..."
    category VARCHAR(50) NOT NULL,  -- politics, economy, etc.
    
    -- Source tracking
    source_url VARCHAR(1000) NOT NULL,
    source_type VARCHAR(50),
    captured_at TIMESTAMP DEFAULT NOW(),
    
    -- Immutability
    content_hash VARCHAR(64) UNIQUE NOT NULL,  -- SHA-256 of claim+source+timestamp
    
    -- Status
    status VARCHAR(50) DEFAULT 'pending_match',  -- pending_match, matched, no_match
    matched_at TIMESTAMP,
    
    -- User interaction
    flagged BOOLEAN DEFAULT FALSE,
    flag_reason TEXT,
    verified_by_pundit BOOLEAN,
    
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_predictions_pundit ON predictions(pundit_id);
CREATE INDEX idx_predictions_status ON predictions(status);
CREATE INDEX idx_predictions_timeframe ON predictions(timeframe);
CREATE INDEX idx_predictions_category ON predictions(category);
CREATE INDEX idx_predictions_hash ON predictions(content_hash);

-- Market matches
CREATE TABLE matches (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    prediction_id UUID REFERENCES predictions(id) NOT NULL,
    
    -- Polymarket market info
    market_id VARCHAR(100) NOT NULL,
    market_slug VARCHAR(255),
    market_question TEXT NOT NULL,
    market_end_date TIMESTAMP,
    
    -- Match quality
    similarity_score FLOAT NOT NULL,
    match_type VARCHAR(50) NOT NULL,  -- auto_matched, manual, needs_review
    alternatives JSONB,  -- Runner-up markets for reference
    
    -- Price at match time
    entry_price FLOAT NOT NULL,
    entry_timestamp TIMESTAMP DEFAULT NOW(),
    
    -- Review (if needed)
    reviewed_by UUID,
    reviewed_at TIMESTAMP,
    
    matched_at TIMESTAMP DEFAULT NOW(),
    
    UNIQUE(prediction_id)  -- One match per prediction
);

CREATE INDEX idx_matches_market ON matches(market_id);
CREATE INDEX idx_matches_type ON matches(match_type);

-- Paper trading positions
CREATE TABLE positions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    prediction_id UUID REFERENCES predictions(id) NOT NULL,
    match_id UUID REFERENCES matches(id) NOT NULL,
    pundit_id UUID REFERENCES pundits(id) NOT NULL,
    
    -- Market info (denormalized for performance)
    market_id VARCHAR(100) NOT NULL,
    market_question TEXT NOT NULL,
    
    -- Position details
    entry_price FLOAT NOT NULL,
    entry_timestamp TIMESTAMP NOT NULL,
    position_size FLOAT NOT NULL,  -- Dollar amount ($100, $500, etc.)
    shares FLOAT NOT NULL,  -- position_size / entry_price
    
    -- Current status
    status VARCHAR(50) DEFAULT 'open',  -- open, closed
    current_price FLOAT,
    unrealized_pnl FLOAT,
    
    -- Resolution
    exit_price FLOAT,
    exit_timestamp TIMESTAMP,
    realized_pnl FLOAT,
    outcome VARCHAR(10),  -- YES, NO
    
    last_updated TIMESTAMP DEFAULT NOW(),
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_positions_pundit ON positions(pundit_id);
CREATE INDEX idx_positions_status ON positions(status);
CREATE INDEX idx_positions_market ON positions(market_id);
CREATE INDEX idx_positions_created ON positions(created_at DESC);

-- Real money positions (verified pundits only)
CREATE TABLE real_positions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    pundit_id UUID REFERENCES pundits(id) NOT NULL,
    prediction_id UUID REFERENCES predictions(id),  -- May be null if position predates tracking
    
    -- Position details (from blockchain/Polymarket API)
    market_id VARCHAR(100) NOT NULL,
    entry_price FLOAT NOT NULL,
    shares FLOAT NOT NULL,
    current_value FLOAT,
    realized_pnl FLOAT,
    status VARCHAR(50) NOT NULL,  -- open, closed
    
    -- Sync metadata
    synced_at TIMESTAMP DEFAULT NOW(),
    blockchain_tx_hash VARCHAR(100),
    
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_real_positions_pundit ON real_positions(pundit_id);
CREATE INDEX idx_real_positions_prediction ON real_positions(prediction_id);

-- Pundit metrics (materialized view for performance)
CREATE TABLE pundit_metrics (
    pundit_id UUID PRIMARY KEY REFERENCES pundits(id),
    
    -- All-time stats
    total_predictions INTEGER DEFAULT 0,
    matched_predictions INTEGER DEFAULT 0,
    resolved_predictions INTEGER DEFAULT 0,
    
    -- Paper trading
    paper_total_pnl FLOAT DEFAULT 0,
    paper_win_rate FLOAT DEFAULT 0,
    paper_roi FLOAT DEFAULT 0,
    paper_sharpe_ratio FLOAT,
    paper_max_drawdown FLOAT,
    
    -- Real money (if verified)
    real_total_pnl FLOAT,
    real_win_rate FLOAT,
    real_roi FLOAT,
    
    -- By category
    metrics_by_category JSONB,  -- {politics: {pnl: X, wins: Y}, economy: {...}}
    
    -- Time-based
    pnl_30d FLOAT,
    pnl_90d FLOAT,
    pnl_365d FLOAT,
    
    -- Rankings
    global_rank INTEGER,
    category_ranks JSONB,
    
    last_calculated TIMESTAMP DEFAULT NOW(),
    
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_pundit_metrics_pnl ON pundit_metrics(paper_total_pnl DESC);
CREATE INDEX idx_pundit_metrics_rank ON pundit_metrics(global_rank);

-- Market cache (Polymarket data)
CREATE TABLE market_cache (
    market_id VARCHAR(100) PRIMARY KEY,
    market_slug VARCHAR(255) UNIQUE,
    question TEXT NOT NULL,
    category VARCHAR(50),
    end_date TIMESTAMP,
    current_price FLOAT,
    volume FLOAT,
    closed BOOLEAN DEFAULT FALSE,
    resolved BOOLEAN DEFAULT FALSE,
    winning_outcome VARCHAR(10),
    
    -- For matching
    embedding VECTOR(1536),  -- Using pgvector extension
    
    fetched_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_market_cache_category ON market_cache(category);
CREATE INDEX idx_market_cache_end_date ON market_cache(end_date);
CREATE INDEX idx_market_cache_active ON market_cache(closed, resolved) WHERE NOT closed;

-- Using pgvector for semantic search
CREATE INDEX ON market_cache USING ivfflat (embedding vector_cosine_ops);

-- Audit log (immutability trail)
CREATE TABLE audit_log (
    id BIGSERIAL PRIMARY KEY,
    table_name VARCHAR(50) NOT NULL,
    record_id UUID NOT NULL,
    action VARCHAR(50) NOT NULL,  -- insert, update, delete
    old_values JSONB,
    new_values JSONB,
    changed_by VARCHAR(100),
    changed_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_audit_log_table ON audit_log(table_name, record_id);
CREATE INDEX idx_audit_log_timestamp ON audit_log(changed_at DESC);
Key Design Decisions:

UUIDs for PKs: Better for distributed systems, no sequential guessing
Denormalization: Copy market info into positions for fast queries
JSONB for flexibility: Store variable metadata without schema changes
Separate real/paper positions: Different guarantees and sync patterns
Materialized metrics: Pre-calculate expensive aggregations
Audit log: Track all changes for accountability
5.2 Data Integrity
Immutability Enforcement:

sql
-- Trigger to prevent prediction modifications
CREATE OR REPLACE FUNCTION prevent_prediction_update()
RETURNS TRIGGER AS $$
BEGIN
    -- Allow only status updates, not content changes
    IF OLD.claim != NEW.claim OR
       OLD.confidence != NEW.confidence OR
       OLD.timeframe != NEW.timeframe OR
       OLD.quote != NEW.quote THEN
        RAISE EXCEPTION 'Predictions cannot be modified after creation';
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER immutable_predictions
BEFORE UPDATE ON predictions
FOR EACH ROW
EXECUTE FUNCTION prevent_prediction_update();

-- Audit log trigger
CREATE OR REPLACE FUNCTION audit_log_changes()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'UPDATE' THEN
        INSERT INTO audit_log (table_name, record_id, action, old_values, new_values)
        VALUES (TG_TABLE_NAME, NEW.id, 'update', 
                row_to_json(OLD), row_to_json(NEW));
    ELSIF TG_OP = 'INSERT' THEN
        INSERT INTO audit_log (table_name, record_id, action, new_values)
        VALUES (TG_TABLE_NAME, NEW.id, 'insert', row_to_json(NEW));
    ELSIF TG_OP = 'DELETE' THEN
        INSERT INTO audit_log (table_name, record_id, action, old_values)
        VALUES (TG_TABLE_NAME, OLD.id, 'delete', row_to_json(OLD));
    END IF;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- Apply audit logging to critical tables
CREATE TRIGGER audit_predictions
AFTER INSERT OR UPDATE OR DELETE ON predictions
FOR EACH ROW
EXECUTE FUNCTION audit_log_changes();

CREATE TRIGGER audit_matches
AFTER INSERT OR UPDATE OR DELETE ON matches
FOR EACH ROW
EXECUTE FUNCTION audit_log_changes();
5.3 Performance Optimization
Materialized View for Leaderboard:

sql
CREATE MATERIALIZED VIEW leaderboard AS
SELECT 
    p.id,
    p.name,
    p.username,
    p.avatar_url,
    p.verified,
    pm.paper_total_pnl,
    pm.paper_win_rate,
    pm.paper_roi,
    pm.total_predictions,
    pm.resolved_predictions,
    pm.global_rank,
    pm.pnl_30d,
    
    -- Recent activity
    COUNT(pos.id) FILTER (WHERE pos.created_at > NOW() - INTERVAL '30 days') as predictions_30d,
    
    -- Best prediction
    (
        SELECT json_build_object(
            'claim', pr.claim,
            'pnl', pos.realized_pnl,
            'market', pos.market_question
        )
        FROM positions pos
        JOIN predictions pr ON pr.id = pos.prediction_id
        WHERE pos.pundit_id = p.id 
          AND pos.status = 'closed'
        ORDER BY pos.realized_pnl DESC
        LIMIT 1
    ) as best_prediction

FROM pundits p
JOIN pundit_metrics pm ON pm.pundit_id = p.id
LEFT JOIN positions pos ON pos.pundit_id = p.id
GROUP BY p.id, pm.paper_total_pnl, pm.paper_win_rate, pm.paper_roi, 
         pm.total_predictions, pm.resolved_predictions, pm.global_rank, pm.pnl_30d
ORDER BY pm.paper_total_pnl DESC;

CREATE UNIQUE INDEX ON leaderboard(id);

-- Refresh every 5 minutes
CREATE OR REPLACE FUNCTION refresh_leaderboard()
RETURNS void AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY leaderboard;
END;
$$ LANGUAGE plpgsql;

-- Schedule via pg_cron
SELECT cron.schedule('refresh-leaderboard', '*/5 * * * *', 'SELECT refresh_leaderboard()');
Partitioning for Audit Log:

sql
-- Partition audit log by month for performance
CREATE TABLE audit_log (
    id BIGSERIAL NOT NULL,
    table_name VARCHAR(50) NOT NULL,
    record_id UUID NOT NULL,
    action VARCHAR(50) NOT NULL,
    old_values JSONB,
    new_values JSONB,
    changed_at TIMESTAMP DEFAULT NOW() NOT NULL,
    PRIMARY KEY (id, changed_at)
) PARTITION BY RANGE (changed_at);

-- Create partitions for current and next 12 months
CREATE TABLE audit_log_2026_01 PARTITION OF audit_log
FOR VALUES FROM ('2026-01-01') TO ('2026-02-01');

CREATE TABLE audit_log_2026_02 PARTITION OF audit_log
FOR VALUES FROM ('2026-02-01') TO ('2026-03-01');

-- ... etc
```

---

## 6. API ARCHITECTURE

### 6.1 API Design

**REST API Endpoints:**
```
Authentication:
POST   /api/auth/login
POST   /api/auth/logout
GET    /api/auth/me

Pundits:
GET    /api/pundits                    # List all pundits
GET    /api/pundits/:id                # Pundit profile
GET    /api/pundits/:id/predictions    # Pundit's predictions
GET    /api/pundits/:id/positions      # Pundit's positions
GET    /api/pundits/:id/metrics        # Pundit's performance metrics

Predictions:
GET    /api/predictions                # List predictions (with filters)
GET    /api/predictions/:id            # Single prediction
POST   /api/predictions/:id/flag       # Flag inaccurate prediction
POST   /api/predictions/:id/verify     # Pundit verifies their prediction

Leaderboard:
GET    /api/leaderboard                # Main leaderboard
GET    /api/leaderboard/category/:cat  # Category-specific leaderboard
GET    /api/leaderboard/timeframe/:tf  # Time-based leaderboard (30d, 90d, etc.)

Markets:
GET    /api/markets                    # List Polymarket markets
GET    /api/markets/:id                # Market details
GET    /api/markets/:id/predictions    # All predictions for this market

Search:
GET    /api/search/pundits?q=          # Search pundits
GET    /api/search/predictions?q=      # Search predictions

Admin (authenticated):
GET    /api/admin/review-queue         # Pending match reviews
POST   /api/admin/matches/:id/approve  # Approve match
POST   /api/admin/matches/:id/reject   # Reject match
GET    /api/admin/metrics              # System metrics

Webhooks (for partners):
POST   /api/webhooks/polymarket        # Receive market updates from Polymarket
API Implementation (FastAPI):

python
from fastapi import FastAPI, Depends, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional, List
import asyncpg

app = FastAPI(
    title="TrackRecord API",
    version="1.0.0",
    description="Pundit prediction tracking and accountability"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://trackrecord.io", "https://app.trackrecord.io"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Database dependency
async def get_db():
    conn = await asyncpg.connect(
        host=os.getenv('DB_HOST'),
        database=os.getenv('DB_NAME'),
        user=os.getenv('DB_USER'),
        password=os.getenv('DB_PASSWORD')
    )
    try:
        yield conn
    finally:
        await conn.close()

# Cache dependency
def get_cache():
    return Redis(host=os.getenv('REDIS_HOST'), port=6379, db=0)

# Models
from pydantic import BaseModel
from datetime import datetime
from typing import Optional

class PunditResponse(BaseModel):
    id: str
    name: str
    username: Optional[str]
    affiliation: Optional[str]
    bio: Optional[str]
    avatar_url: Optional[str]
    domains: List[str]
    verified: bool
    metrics: 'MetricsResponse'

class MetricsResponse(BaseModel):
    total_predictions: int
    resolved_predictions: int
    paper_total_pnl: float
    paper_win_rate: float
    paper_roi: float
    real_total_pnl: Optional[float]
    pnl_30d: float
    global_rank: int

class PredictionResponse(BaseModel):
    id: str
    pundit: 'PunditSummary'
    claim: str
    confidence: float
    timeframe: datetime
    quote: str
    category: str
    source_url: str
    captured_at: datetime
    status: str
    position: Optional['PositionSummary']

class PositionResponse(BaseModel):
    id: str
    market_question: str
    entry_price: float
    position_size: float
    shares: float
    status: str
    current_price: Optional[float]
    unrealized_pnl: Optional[float]
    realized_pnl: Optional[float]
    outcome: Optional[str]

# Endpoints

@app.get("/api/leaderboard", response_model=List[PunditResponse])
async def get_leaderboard(
    category: Optional[str] = None,
    timeframe: str = 'all',  # all, 30d, 90d, 365d
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
    db = Depends(get_db),
    cache = Depends(get_cache)
):
    """
    Get leaderboard of pundits ranked by performance
    """
    # Check cache first
    cache_key = f"leaderboard:{category}:{timeframe}:{limit}:{offset}"
    cached = await cache.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    # Build query based on filters
    query = """
        SELECT 
            p.id, p.name, p.username, p.affiliation, p.bio, 
            p.avatar_url, p.domains, p.verified,
            pm.*
        FROM pundits p
        JOIN pundit_metrics pm ON pm.pundit_id = p.id
        WHERE 1=1
    """
    params = []
    
    if category:
        query += " AND ? = ANY(p.domains)"
        params.append(category)
    
    # Order by timeframe
    if timeframe == '30d':
        query += " ORDER BY pm.pnl_30d DESC"
    elif timeframe == '90d':
        query += " ORDER BY pm.pnl_90d DESC"
    elif timeframe == '365d':
        query += " ORDER BY pm.pnl_365d DESC"
    else:
        query += " ORDER BY pm.paper_total_pnl DESC"
    
    query += f" LIMIT {limit} OFFSET {offset}"
    
    rows = await db.fetch(query, *params)
    
    pundits = [PunditResponse(**dict(row)) for row in rows]
    
    # Cache for 5 minutes
    await cache.setex(cache_key, 300, json.dumps([p.dict() for p in pundits]))
    
    return pundits

@app.get("/api/pundits/{pundit_id}", response_model=PunditResponse)
async def get_pundit(
    pundit_id: str,
    db = Depends(get_db),
    cache = Depends(get_cache)
):
    """
    Get detailed pundit profile
    """
    cache_key = f"pundit:{pundit_id}"
    cached = await cache.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    row = await db.fetchrow("""
        SELECT 
            p.*, pm.*
        FROM pundits p
        LEFT JOIN pundit_metrics pm ON pm.pundit_id = p.id
        WHERE p.id = $1
    """, pundit_id)
    
    if not row:
        raise HTTPException(status_code=404, detail="Pundit not found")
    
    pundit = PunditResponse(**dict(row))
    
    # Cache for 1 minute (pundit data changes frequently)
    await cache.setex(cache_key, 60, json.dumps(pundit.dict()))
    
    return pundit

@app.get("/api/pundits/{pundit_id}/predictions", response_model=List[PredictionResponse])
async def get_pundit_predictions(
    pundit_id: str,
    status: Optional[str] = None,  # pending_match, matched, resolved
    category: Optional[str] = None,
    limit: int = Query(50, ge=1, le=200),
    offset: int = Query(0, ge=0),
    db = Depends(get_db)
):
    """
    Get all predictions for a pundit
    """
    query = """
        SELECT 
            pred.*,
            pos.id as position_id,
            pos.status as position_status,
            pos.unrealized_pnl,
            pos.realized_pnl
        FROM predictions pred
        LEFT JOIN matches m ON m.prediction_id = pred.id
        LEFT JOIN positions pos ON pos.prediction_id = pred.id
        WHERE pred.pundit_id = $1
    """
    params = [pundit_id]
    param_count = 1
    
    if status:
        param_count += 1
        query += f" AND pred.status = ${param_count}"
        params.append(status)
    
    if category:
        param_count += 1
        query += f" AND pred.category = ${param_count}"
        params.append(category)
    
    query += f" ORDER BY pred.captured_at DESC LIMIT {limit} OFFSET {offset}"
    
    rows = await db.fetch(query, *params)
    
    predictions = [PredictionResponse(**dict(row)) for row in rows]
    
    return predictions

@app.get("/api/predictions/{prediction_id}", response_model=PredictionResponse)
async def get_prediction(
    prediction_id: str,
    db = Depends(get_db)
):
    """
    Get single prediction with full details
    """
    row = await db.fetchrow("""
        SELECT 
            pred.*,
            p.name as pundit_name,
            p.username as pundit_username,
            p.avatar_url as pundit_avatar,
            m.market_id,
            m.market_question,
            m.similarity_score,
            pos.*
        FROM predictions pred
        JOIN pundits p ON p.id = pred.pundit_id
        LEFT JOIN matches m ON m.prediction_id = pred.id
        LEFT JOIN positions pos ON pos.prediction_id = pred.id
        WHERE pred.id = $1
    """, prediction_id)
    
    if not row:
        raise HTTPException(status_code=404, detail="Prediction not found")
    
    return PredictionResponse(**dict(row))

@app.get("/api/search/pundits")
async def search_pundits(
    q: str = Query(..., min_length=2),
    limit: int = Query(10, ge=1, le=50),
    db = Depends(get_db)
):
    """
    Search pundits by name
    """
    rows = await db.fetch("""
        SELECT id, name, username, avatar_url, affiliation, verified
        FROM pundits
        WHERE name ILIKE $1 OR username ILIKE $1
        LIMIT $2
    """, f"%{q}%", limit)
    
    return [dict(row) for row in rows]

@app.post("/api/predictions/{prediction_id}/flag")
async def flag_prediction(
    prediction_id: str,
    reason: str,
    user_id: Optional[str] = None,  # From auth
    db = Depends(get_db)
):
    """
    Flag a prediction as inaccurate or not a prediction
    """
    await db.execute("""
        UPDATE predictions
        SET flagged = TRUE,
            flag_reason = $2
        WHERE id = $1
    """, prediction_id, reason)
    
    # Notify admins
    await notify_admin_flag(prediction_id, reason)
    
    return {"success": True}

# Admin endpoints (require authentication)

@app.get("/api/admin/review-queue")
async def get_review_queue(
    limit: int = Query(50, ge=1, le=200),
    db = Depends(get_db),
    current_user = Depends(require_admin)  # Auth check
):
    """
    Get predictions needing manual review for market matching
    """
    rows = await db.fetch("""
        SELECT 
            m.*,
            pred.claim,
            pred.quote,
            pred.captured_at,
            p.name as pundit_name
        FROM match_review_queue m
        JOIN predictions pred ON pred.id = m.prediction_id
        JOIN pundits p ON p.id = pred.pundit_id
        WHERE m.status = 'pending'
        ORDER BY m.created_at ASC
        LIMIT $1
    """, limit)
    
    return [dict(row) for row in rows]

@app.post("/api/admin/matches/{match_id}/approve")
async def approve_match(
    match_id: str,
    db = Depends(get_db),
    current_user = Depends(require_admin)
):
    """
    Admin approves a market match
    """
    # Get match details
    match = await db.fetchrow("""
        SELECT * FROM match_review_queue WHERE id = $1
    """, match_id)
    
    if not match:
        raise HTTPException(status_code=404, detail="Match not found")
    
    # Create position
    position_simulator = PositionSimulator()
    await position_simulator.create_position(match, match['prediction_id'])
    
    # Mark as reviewed
    await db.execute("""
        UPDATE match_review_queue
        SET status = 'approved',
            reviewed_by = $2,
            reviewed_at = NOW()
        WHERE id = $1
    """, match_id, current_user.id)
    
    return {"success": True}
6.2 Rate Limiting
python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# Apply rate limits
@app.get("/api/leaderboard")
@limiter.limit("100/minute")  # Generous for public endpoint
async def get_leaderboard(...):
    pass

@app.post("/api/predictions/{id}/flag")
@limiter.limit("10/hour")  # Prevent abuse
async def flag_prediction(...):
    pass
6.3 API Authentication
python
from jose import JWTError, jwt
from datetime import datetime, timedelta
from passlib.context import CryptContext

SECRET_KEY = os.getenv("JWT_SECRET_KEY")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

async def get_current_user(token: str = Depends(oauth2_scheme), db = Depends(get_db)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id: str = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401, detail="Invalid token")
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")
    
    user = await db.fetchrow("SELECT * FROM users WHERE id = $1", user_id)
    if user is None:
        raise HTTPException(status_code=401, detail="User not found")
    
    return user

def require_admin(current_user = Depends(get_current_user)):
    if not current_user['is_admin']:
        raise HTTPException(status_code=403, detail="Admin access required")
    return current_user
```

---

## 7. FRONTEND ARCHITECTURE

### 7.1 Technology Stack

**Framework:** Next.js 14 (App Router)
**Styling:** Tailwind CSS
**State Management:** React Query + Zustand
**Charts:** Recharts
**UI Components:** shadcn/ui
**Deployment:** Vercel

### 7.2 Project Structure
```
/app
  /(public)
    /page.tsx                  # Homepage / Leaderboard
    /pundits/[id]/page.tsx    # Pundit profile
    /predictions/[id]/page.tsx # Single prediction
    /markets/[id]/page.tsx     # Market detail
    /about/page.tsx            # About page
  
  /api
    /predictions/route.ts      # API routes (if needed)
  
  /dashboard
    /page.tsx                  # Admin dashboard
    /review/page.tsx           # Match review queue

/components
  /leaderboard
    /LeaderboardTable.tsx
    /PunditCard.tsx
  /predictions
    /PredictionCard.tsx
    /PredictionTimeline.tsx
  /charts
    /PnLChart.tsx
    /WinRateChart.tsx
    /CalibrationCurve.tsx
  /ui                          # shadcn components
    /button.tsx
    /card.tsx
    /...

/lib
  /api.ts                      # API client
  /hooks.ts                    # Custom React hooks
  /utils.ts                    # Utilities

/styles
  /globals.css                 # Global styles

/public
  /images
  /icons
7.3 Key Components
Leaderboard Component:

tsx
'use client'

import { useQuery } from '@tanstack/react-query'
import { useState } from 'react'
import { api } from '@/lib/api'
import { PunditCard } from '@/components/leaderboard/PunditCard'
import { Tabs, TabsList, TabsTrigger } from '@/components/ui/tabs'

export function Leaderboard() {
  const [category, setCategory] = useState<string | null>(null)
  const [timeframe, setTimeframe] = useState('all')
  
  const { data: pundits, isLoading } = useQuery({
    queryKey: ['leaderboard', category, timeframe],
    queryFn: () => api.getLeaderboard({ category, timeframe }),
    staleTime: 60 * 1000, // 1 minute
  })
  
  if (isLoading) {
    return <LeaderboardSkeleton />
  }
  
  return (
    <div className="container mx-auto px-4 py-8">
      <h1 className="text-4xl font-bold mb-8">
        Pundit Leaderboard
      </h1>
      
      {/* Filters */}
      <div className="flex gap-4 mb-8">
        <Tabs value={timeframe} onValueChange={setTimeframe}>
          <TabsList>
            <TabsTrigger value="all">All Time</TabsTrigger>
            <TabsTrigger value="30d">Last 30 Days</TabsTrigger>
            <TabsTrigger value="90d">Last 90 Days</TabsTrigger>
            <TabsTrigger value="365d">Last Year</TabsTrigger>
          </TabsList>
        </Tabs>
        
        <Tabs value={category || 'all'} onValueChange={(v) => setCategory(v === 'all' ? null : v)}>
          <TabsList>
            <TabsTrigger value="all">All Categories</TabsTrigger>
            <TabsTrigger value="politics">Politics</TabsTrigger>
            <TabsTrigger value="economy">Economy</TabsTrigger>
            <TabsTrigger value="crypto">Crypto</TabsTrigger>
            <TabsTrigger value="tech">Tech</TabsTrigger>
          </TabsList>
        </Tabs>
      </div>
      
      {/* Leaderboard Table */}
      <div className="grid gap-4">
        {pundits?.map((pundit, index) => (
          <PunditCard
            key={pundit.id}
            pundit={pundit}
            rank={index + 1}
          />
        ))}
      </div>
    </div>
  )
}
Pundit Card Component:

tsx
import Link from 'next/link'
import Image from 'next/image'
import { formatCurrency, formatPercent } from '@/lib/utils'
import { Badge } from '@/components/ui/badge'
import { TrendingUp, TrendingDown } from 'lucide-react'

interface PunditCardProps {
  pundit: Pundit
  rank: number
}

export function PunditCard({ pundit, rank }: PunditCardProps) {
  const isProfit = pundit.metrics.paper_total_pnl > 0
  
  return (
    <Link href={`/pundits/${pundit.id}`}>
      <div className="flex items-center gap-4 p-4 border rounded-lg hover:bg-gray-50 transition-colors">
        {/* Rank */}
        <div className="text-2xl font-bold text-gray-400 w-12">
          #{rank}
        </div>
        
        {/* Avatar */}
        <Image
          src={pundit.avatar_url || '/default-avatar.png'}
          alt={pundit.name}
          width={64}
          height={64}
          className="rounded-full"
        />
        
        {/* Info */}
        <div className="flex-1">
          <div className="flex items-center gap-2">
            <h3 className="text-lg font-semibold">{pundit.name}</h3>
            {pundit.verified && (
              <Badge variant="secondary">✅ Verified</Badge>
            )}
          </div>
          <p className="text-sm text-gray-600">
            @{pundit.username} • {pundit.affiliation}
          </p>
          <div className="flex gap-4 mt-2 text-sm">
            <span>{pundit.metrics.total_predictions} predictions</span>
            <span>{pundit.metrics.resolved_predictions} resolved</span>
            <span>{formatPercent(pundit.metrics.paper_win_rate)} win rate</span>
          </div>
        </div>
        
        {/* P&L */}
        <div className="text-right">
          <div className={`text-2xl font-bold ${isProfit ? 'text-green-600' : 'text-red-600'}`}>
            {isProfit ? <TrendingUp className="inline mr-1" /> : <TrendingDown className="inline mr-1" />}
            {formatCurrency(pundit.metrics.paper_total_pnl)}
          </div>
          <div className="text-sm text-gray-600">
            {formatPercent(pundit.metrics.paper_roi)} ROI
          </div>
        </div>
      </div>
    </Link>
  )
}
Pundit Profile Page:

tsx
import { api } from '@/lib/api'
import { PnLChart } from '@/components/charts/PnLChart'
import { PredictionsList } from '@/components/predictions/PredictionsList'
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs'

export default async function PunditPage({ params }: { params: { id: string } }) {
  const pundit = await api.getPundit(params.id)
  const predictions = await api.getPunditPredictions(params.id)
  
  return (
    <div className="container mx-auto px-4 py-8">
      {/* Header */}
      <div className="flex items-start gap-6 mb-8">
        <Image
          src={pundit.avatar_url}
          alt={pundit.name}
          width={128}
          height={128}
          className="rounded-full"
        />
        <div className="flex-1">
          <div className="flex items-center gap-3 mb-2">
            <h1 className="text-4xl font-bold">{pundit.name}</h1>
            {pundit.verified && (
              <Badge className="bg-green-600">✅ Verified</Badge>
            )}
          </div>
          <p className="text-xl text-gray-600 mb-2">
            @{pundit.username} • {pundit.affiliation}
          </p>
          <p className="text-gray-700">{pundit.bio}</p>
          
          {/* Quick Stats */}
          <div className="grid grid-cols-4 gap-4 mt-6">
            <StatCard
              label="Total P&L"
              value={formatCurrency(pundit.metrics.paper_total_pnl)}
              trend={pundit.metrics.paper_total_pnl > 0 ? 'up' : 'down'}
            />
            <StatCard
              label="Win Rate"
              value={formatPercent(pundit.metrics.paper_win_rate)}
            />
            <StatCard
              label="Predictions"
              value={pundit.metrics.total_predictions}
            />
            <StatCard
              label="Global Rank"
              value={`#${pundit.metrics.global_rank}`}
            />
          </div>
        </div>
      </div>
      
      {/* Tabs */}
      <Tabs defaultValue="overview">
        <TabsList>
          <TabsTrigger value="overview">Overview</TabsTrigger>
          <TabsTrigger value="predictions">Predictions</TabsTrigger>
          <TabsTrigger value="positions">Open Positions</TabsTrigger>
          {pundit.verified && (
            <TabsTrigger value="real-money">Real Money</TabsTrigger>
          )}
        </TabsList>
        
        <TabsContent value="overview">
          <div className="grid grid-cols-2 gap-6">
            <PnLChart punditId={pundit.id} />
            <WinRateByCategory punditId={pundit.id} />
            <CalibrationCurve punditId={pundit.id} />
            <RecentPredictions predictions={predictions.slice(0, 5)} />
          </div>
        </TabsContent>
        
        <TabsContent value="predictions">
          <PredictionsList predictions={predictions} />
        </TabsContent>
        
        <TabsContent value="positions">
          <OpenPositionsList punditId={pundit.id} />
        </TabsContent>
        
        {pundit.verified && (
          <TabsContent value="real-money">
            <RealMoneyPositions punditId={pundit.id} />
          </TabsContent>
        )}
      </Tabs>
    </div>
  )
}
7.4 Data Fetching
typescript
// lib/api.ts

const API_BASE_URL = process.env.NEXT_PUBLIC_API_URL || 'https://api.trackrecord.io'

export const api = {
  async getLeaderboard(filters?: {
    category?: string | null
    timeframe?: string
    limit?: number
    offset?: number
  }) {
    const params = new URLSearchParams()
    if (filters?.category) params.append('category', filters.category)
    if (filters?.timeframe) params.append('timeframe', filters.timeframe)
    if (filters?.limit) params.append('limit', String(filters.limit))
    if (filters?.offset) params.append('offset', String(filters.offset))
    
    const res = await fetch(`${API_BASE_URL}/api/leaderboard?${params}`)
    return res.json()
  },
  
  async getPundit(id: string) {
    const res = await fetch(`${API_BASE_URL}/api/pundits/${id}`)
    return res.json()
  },
  
  async getPunditPredictions(id: string, filters?: {
    status?: string
    category?: string
    limit?: number
    offset?: number
  }) {
    const params = new URLSearchParams()
    if (filters?.status) params.append('status', filters.status)
    if (filters?.category) params.append('category', filters.category)
    if (filters?.limit) params.append('limit', String(filters.limit))
    if (filters?.offset) params.append('offset', String(filters.offset))
    
    const res = await fetch(`${API_BASE_URL}/api/pundits/${id}/predictions?${params}`)
    return res.json()
  },
  
  async getPrediction(id: string) {
    const res = await fetch(`${API_BASE_URL}/api/predictions/${id}`)
    return res.json()
  },
  
  async searchPundits(query: string) {
    const params = new URLSearchParams({ q: query })
    const res = await fetch(`${API_BASE_URL}/api/search/pundits?${params}`)
    return res.json()
  },
  
  async flagPrediction(id: string, reason: string) {
    const res = await fetch(`${API_BASE_URL}/api/predictions/${id}/flag`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ reason })
    })
    return res.json()
  }
}
8. INFRASTRUCTURE & DEVOPS
8.1 Infrastructure Architecture
┌─────────────────────────────────────────────────────────────────┐
│                        CLOUDFLARE                                │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐                │
│  │    CDN     │  │    WAF     │  │   DDoS     │                │
│  │  (Static)  │  │ (Security) │  │ Protection │                │
│  └────────────┘  └────────────┘  └────────────┘                │
└───────────────────────┬─────────────────────────────────────────┘
                        │
        ┌───────────────┴───────────────┐
        │                               │
┌───────▼────────┐            ┌─────────▼──────┐
│    VERCEL      │            │      AWS       │
│   (Frontend)   │            │   (Backend)    │
│  - Next.js App │            │                │
│  - Edge Funcs  │            │                │
└────────────────┘            └────────
Continue

1:37 PM
┬───────┘
│
┌──────────────────┼──────────────────┐
│                  │                  │
┌───────▼────────┐ ┌──────▼──────┐  ┌───────▼────────┐
│   EC2 (API)    │ │  RDS (PG)   │  │  ElastiCache  │
│   - FastAPI    │ │  - Primary  │  │   (Redis)     │
│   - Gunicorn   │ │  - Replica  │  │               │
│   - Auto-scale │ │             │  │               │
└───────┬────────┘ └─────────────┘  └───────────────┘
│
┌───────▼────────┐
│   S3 Buckets   │
│  - Raw data    │
│  - Backups     │
│  - Static      │
└────────────────┘


### 8.2 AWS Setup

**Terraform Configuration:**
```hcl
# main.tf

terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {
    bucket = "trackrecord-terraform-state"
    key    = "prod/terraform.tfstate"
    region = "us-east-1"
  }
}

provider "aws" {
  region = "us-east-1"
}

# VPC
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"
  
  tags = {
    Name = "trackrecord-vpc"
  }
}

# Subnets
resource "aws_subnet" "public_a" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.1.0/24"
  availability_zone = "us-east-1a"
  
  tags = {
    Name = "trackrecord-public-a"
  }
}

resource "aws_subnet" "public_b" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.2.0/24"
  availability_zone = "us-east-1b"
  
  tags = {
    Name = "trackrecord-public-b"
  }
}

resource "aws_subnet" "private_a" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.3.0/24"
  availability_zone = "us-east-1a"
  
  tags = {
    Name = "trackrecord-private-a"
  }
}

resource "aws_subnet" "private_b" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.4.0/24"
  availability_zone = "us-east-1b"
  
  tags = {
    Name = "trackrecord-private-b"
  }
}

# RDS PostgreSQL
resource "aws_db_instance" "main" {
  identifier             = "trackrecord-db"
  engine                 = "postgres"
  engine_version         = "15.4"
  instance_class         = "db.t3.medium"
  allocated_storage      = 100
  storage_type           = "gp3"
  
  db_name  = "trackrecord"
  username = var.db_username
  password = var.db_password
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = 7
  backup_window           = "03:00-04:00"
  maintenance_window      = "mon:04:00-mon:05:00"
  
  multi_az               = true  # High availability
  publicly_accessible    = false
  skip_final_snapshot    = false
  final_snapshot_identifier = "trackrecord-final-${timestamp()}"
  
  enabled_cloudwatch_logs_exports = ["postgresql", "upgrade"]
  
  tags = {
    Name = "trackrecord-db"
  }
}

# ElastiCache Redis
resource "aws_elasticache_cluster" "main" {
  cluster_id           = "trackrecord-cache"
  engine               = "redis"
  engine_version       = "7.0"
  node_type            = "cache.t3.medium"
  num_cache_nodes      = 1
  parameter_group_name = "default.redis7"
  port                 = 6379
  
  subnet_group_name    = aws_elasticache_subnet_group.main.name
  security_group_ids   = [aws_security_group.redis.id]
  
  tags = {
    Name = "trackrecord-cache"
  }
}

# EC2 Auto Scaling Group for API
resource "aws_launch_template" "api" {
  name_prefix   = "trackrecord-api-"
  image_id      = "ami-0c55b159cbfafe1f0"  # Amazon Linux 2023
  instance_type = "t3.medium"
  
  user_data = base64encode(templatefile("${path.module}/api-user-data.sh", {
    db_host     = aws_db_instance.main.address
    redis_host  = aws_elasticache_cluster.main.cache_nodes[0].address
  }))
  
  iam_instance_profile {
    name = aws_iam_instance_profile.api.name
  }
  
  vpc_security_group_ids = [aws_security_group.api.id]
  
  tag_specifications {
    resource_type = "instance"
    tags = {
      Name = "trackrecord-api"
    }
  }
}

resource "aws_autoscaling_group" "api" {
  desired_capacity    = 2
  max_size            = 10
  min_size            = 2
  vpc_zone_identifier = [aws_subnet.private_a.id, aws_subnet.private_b.id]
  
  launch_template {
    id      = aws_launch_template.api.id
    version = "$Latest"
  }
  
  target_group_arns = [aws_lb_target_group.api.arn]
  
  tag {
    key                 = "Name"
    value               = "trackrecord-api"
    propagate_at_launch = true
  }
}

# Application Load Balancer
resource "aws_lb" "api" {
  name               = "trackrecord-api-lb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.alb.id]
  subnets            = [aws_subnet.public_a.id, aws_subnet.public_b.id]
  
  tags = {
    Name = "trackrecord-api-lb"
  }
}

resource "aws_lb_target_group" "api" {
  name     = "trackrecord-api-tg"
  port     = 8000
  protocol = "HTTP"
  vpc_id   = aws_vpc.main.id
  
  health_check {
    path                = "/health"
    healthy_threshold   = 2
    unhealthy_threshold = 3
    timeout             = 5
    interval            = 30
  }
}

resource "aws_lb_listener" "api" {
  load_balancer_arn = aws_lb.api.arn
  port              = "443"
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-TLS13-1-2-2021-06"
  certificate_arn   = aws_acm_certificate.api.arn
  
  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.api.arn
  }
}

# S3 Buckets
resource "aws_s3_bucket" "data" {
  bucket = "trackrecord-data-${var.environment}"
  
  tags = {
    Name = "trackrecord-data"
  }
}

resource "aws_s3_bucket_versioning" "data" {
  bucket = aws_s3_bucket.data.id
  
  versioning_configuration {
    status = "Enabled"
  }
}

# CloudWatch Log Groups
resource "aws_cloudwatch_log_group" "api" {
  name              = "/aws/ec2/trackrecord-api"
  retention_in_days = 30
}

# Auto Scaling Policies
resource "aws_autoscaling_policy" "scale_up" {
  name                   = "trackrecord-scale-up"
  scaling_adjustment     = 2
  adjustment_type        = "ChangeInCapacity"
  cooldown               = 300
  autoscaling_group_name = aws_autoscaling_group.api.name
}

resource "aws_autoscaling_policy" "scale_down" {
  name                   = "trackrecord-scale-down"
  scaling_adjustment     = -1
  adjustment_type        = "ChangeInCapacity"
  cooldown               = 300
  autoscaling_group_name = aws_autoscaling_group.api.name
}

# CloudWatch Alarms
resource "aws_cloudwatch_metric_alarm" "high_cpu" {
  alarm_name          = "trackrecord-high-cpu"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = "300"
  statistic           = "Average"
  threshold           = "70"
  
  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.api.name
  }
  
  alarm_actions = [aws_autoscaling_policy.scale_up.arn]
}
```

**User Data Script (api-user-data.sh):**
```bash
#!/bin/bash
set -e

# Update system
yum update -y

# Install Python 3.11
amazon-linux-extras install python3.11 -y

# Install Docker
yum install docker -y
systemctl start docker
systemctl enable docker

# Pull and run API container
docker pull trackrecord/api:latest

docker run -d \
  --name trackrecord-api \
  --restart always \
  -p 8000:8000 \
  -e DB_HOST=${db_host} \
  -e REDIS_HOST=${redis_host} \
  -e AWS_REGION=us-east-1 \
  trackrecord/api:latest

# Install CloudWatch agent
wget https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm
rpm -U ./amazon-cloudwatch-agent.rpm

# Configure CloudWatch logs
cat > /opt/aws/amazon-cloudwatch-agent/etc/config.json <<EOF
{
  "logs": {
    "logs_collected": {
      "files": {
        "collect_list": [
          {
            "file_path": "/var/log/trackrecord/*.log",
            "log_group_name": "/aws/ec2/trackrecord-api",
            "log_stream_name": "{instance_id}"
          }
        ]
      }
    }
  }
}
EOF

/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \
  -a fetch-config \
  -m ec2 \
  -c file:/opt/aws/amazon-cloudwatch-agent/etc/config.json \
  -s
```

### 8.3 Docker Setup

**Dockerfile:**
```dockerfile
# Dockerfile

FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY ./app /app

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8000

# Run with Gunicorn
CMD ["gunicorn", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000", "main:app"]
```

**docker-compose.yml (for local development):**
```yaml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DB_HOST=postgres
      - DB_NAME=trackrecord
      - DB_USER=trackrecord
      - DB_PASSWORD=trackrecord
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - postgres
      - redis
    volumes:
      - ./app:/app
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
  
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=trackrecord
      - POSTGRES_USER=trackrecord
      - POSTGRES_PASSWORD=trackrecord
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
  
  worker:
    build: .
    command: celery -A tasks worker --loglevel=info
    environment:
      - DB_HOST=postgres
      - REDIS_HOST=redis
    depends_on:
      - postgres
      - redis
    volumes:
      - ./app:/app

volumes:
  postgres_data:
  redis_data:
```

### 8.4 CI/CD Pipeline

**GitHub Actions (.github/workflows/deploy.yml):**
```yaml
name: Deploy

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run tests
        env:
          DB_HOST: localhost
          REDIS_HOST: localhost
        run: |
          pytest --cov=app tests/
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
  
  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
      
      - name: Build and push Docker image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: trackrecord-api
          IMAGE_TAG: ${{ github.sha }}
        run: |
          docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
          docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG $ECR_REGISTRY/$ECR_REPOSITORY:latest
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
  
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to EC2
        env:
          PRIVATE_KEY: ${{ secrets.EC2_SSH_KEY }}
          HOST: ${{ secrets.EC2_HOST }}
          USER: ec2-user
        run: |
          echo "$PRIVATE_KEY" > private_key && chmod 600 private_key
          ssh -o StrictHostKeyChecking=no -i private_key ${USER}@${HOST} '
            cd /opt/trackrecord
            docker pull trackrecord/api:latest
            docker-compose down
            docker-compose up -d
            docker system prune -af
          '
```

---

## 9. SECURITY & PRIVACY

### 9.1 Security Measures

**API Security:**
```python
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi import Security, HTTPException
import jwt

security = HTTPBearer()

async def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):
    """
    Verify JWT token for authenticated endpoints
    """
    try:
        payload = jwt.decode(
            credentials.credentials,
            os.getenv('JWT_SECRET'),
            algorithms=["HS256"]
        )
        return payload
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="Token expired")
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401, detail="Invalid token")

# Rate limiting per user
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/api/predictions/{id}/flag")
@limiter.limit("5/hour")  # Prevent abuse
async def flag_prediction(
    id: str,
    reason: str,
    user = Depends(verify_token)
):
    pass
```

**Database Security:**
```python
# Encrypted wallet addresses
from cryptography.fernet import Fernet
import os

class EncryptionService:
    def __init__(self):
        self.key = os.getenv('ENCRYPTION_KEY').encode()
        self.cipher = Fernet(self.key)
    
    def encrypt(self, data: str) -> bytes:
        return self.cipher.encrypt(data.encode())
    
    def decrypt(self, encrypted: bytes) -> str:
        return self.cipher.decrypt(encrypted).decode()

# Usage
encryption = EncryptionService()

async def store_wallet(pundit_id: str, wallet_address: str):
    encrypted_wallet = encryption.encrypt(wallet_address)
    await db.execute("""
        UPDATE pundits
        SET wallet_address = $1
        WHERE id = $2
    """, encrypted_wallet, pundit_id)

async def get_wallet(pundit_id: str) -> str:
    row = await db.fetchrow("""
        SELECT wallet_address FROM pundits WHERE id = $1
    """, pundit_id)
    
    if row and row['wallet_address']:
        return encryption.decrypt(row['wallet_address'])
    return None
```

**SQL Injection Prevention:**
```python
# ALWAYS use parameterized queries
# BAD:
query = f"SELECT * FROM pundits WHERE name = '{name}'"  # ❌ SQL injection risk

# GOOD:
query = "SELECT * FROM pundits WHERE name = $1"  # ✅ Parameterized
result = await db.fetch(query, name)
```

**CORS Configuration:**
```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://trackrecord.io",
        "https://www.trackrecord.io",
        "https://app.trackrecord.io"
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
    max_age=3600,
)
```

### 9.2 Privacy Considerations

**GDPR Compliance:**

- Users can request data deletion (pundit profiles can be "anonymized")
- Clear privacy policy explaining data collection
- Cookie consent banner
- Right to access personal data
- Right to be forgotten

**User Data:**
```python
@app.delete("/api/users/me")
async def delete_user(user = Depends(verify_token)):
    """
    GDPR right to be forgotten
    """
    # Anonymize instead of delete (preserve prediction integrity)
    await db.execute("""
        UPDATE users
        SET 
            email = 'deleted@trackrecord.io',
            name = 'Deleted User',
            deleted_at = NOW()
        WHERE id = $1
    """, user['id'])
    
    return {"success": True}
```

---

## 10. SCALABILITY & PERFORMANCE

### 10.1 Caching Strategy

**Multi-Layer Caching:**
```python
class CacheService:
    """
    Multi-layer caching: Memory → Redis → Database
    """
    
    def __init__(self):
        self.memory_cache = {}  # In-process cache
        self.redis = Redis()
        self.db = Database()
    
    async def get_leaderboard(self, category: str, timeframe: str):
        """
        Get leaderboard with multi-layer caching
        """
        cache_key = f"leaderboard:{category}:{timeframe}"
        
        # Layer 1: Memory cache (instant)
        if cache_key in self.memory_cache:
            return self.memory_cache[cache_key]
        
        # Layer 2: Redis cache (fast)
        cached = await self.redis.get(cache_key)
        if cached:
            data = json.loads(cached)
            self.memory_cache[cache_key] = data
            return data
        
        # Layer 3: Database (slow)
        data = await self.db.fetch_leaderboard(category, timeframe)
        
        # Cache in Redis for 5 minutes
        await self.redis.setex(cache_key, 300, json.dumps(data))
        
        # Cache in memory for 1 minute
        self.memory_cache[cache_key] = data
        asyncio.create_task(self.expire_memory_cache(cache_key, 60))
        
        return data
    
    async def expire_memory_cache(self, key: str, seconds: int):
        await asyncio.sleep(seconds)
        self.memory_cache.pop(key, None)
    
    async def invalidate(self, pattern: str):
        """
        Invalidate caches matching pattern
        """
        # Clear memory cache
        keys_to_delete = [k for k in self.memory_cache.keys() if pattern in k]
        for key in keys_to_delete:
            del self.memory_cache[key]
        
        # Clear Redis cache
        keys = await self.redis.keys(f"*{pattern}*")
        if keys:
            await self.redis.delete(*keys)
```

**Cache Invalidation Strategy:**
```python
# When a position resolves, invalidate relevant caches
async def on_position_resolved(position: Position):
    """
    Invalidate caches when position resolves
    """
    cache = CacheService()
    
    # Invalidate pundit-specific caches
    await cache.invalidate(f"pundit:{position.pundit_id}")
    
    # Invalidate leaderboard caches
    await cache.invalidate("leaderboard:")
    
    # Invalidate market-specific caches
    await cache.invalidate(f"market:{position.market_id}")
```

### 10.2 Database Optimization

**Connection Pooling:**
```python
from asyncpg import create_pool

class Database:
    def __init__(self):
        self.pool = None
    
    async def connect(self):
        self.pool = await create_pool(
            host=os.getenv('DB_HOST'),
            database=os.getenv('DB_NAME'),
            user=os.getenv('DB_USER'),
            password=os.getenv('DB_PASSWORD'),
            min_size=10,  # Minimum connections
            max_size=50,  # Maximum connections
            command_timeout=60,
            max_queries=50000,  # Recycle connections
            max_inactive_connection_lifetime=300
        )
    
    async def fetch(self, query: str, *args):
        async with self.pool.acquire() as conn:
            return await conn.fetch(query, *args)
```

**Query Optimization:**
```sql
-- Create indexes for common queries
CREATE INDEX CONCURRENTLY idx_positions_pundit_status 
    ON positions(pundit_id, status);

CREATE INDEX CONCURRENTLY idx_predictions_captured_desc 
    ON predictions(captured_at DESC);

CREATE INDEX CONCURRENTLY idx_positions_pnl 
    ON positions(realized_pnl DESC NULLS LAST) 
    WHERE status = 'closed';

-- Analyze query plans
EXPLAIN ANALYZE
SELECT p.*, pm.*
FROM pundits p
JOIN pundit_metrics pm ON pm.pundit_id = p.id
ORDER BY pm.paper_total_pnl DESC
LIMIT 20;
```

### 10.3 Async Processing

**Background Jobs with Celery:**
```python
from celery import Celery

celery = Celery('trackrecord', broker=os.getenv('REDIS_URL'))

@celery.task
def process_prediction(raw_content_id: str):
    """
    Background task: Extract predictions from raw content
    """
    extractor = PredictionExtractor()
    raw_content = db.get_raw_content(raw_content_id)
    predictions = extractor.extract(raw_content)
    
    for prediction in predictions:
        db.insert_prediction(prediction)
        
        # Enqueue for matching
        match_prediction.delay(prediction.id)

@celery.task
def match_prediction(prediction_id: str):
    """
    Background task: Match prediction to market
    """
    matcher = MarketMatcher()
    prediction = db.get_prediction(prediction_id)
    match = matcher.match(prediction)
    
    if match:
        db.insert_match(match)
        
        # Create position
        create_position.delay(match.id)

@celery.task
def create_position(match_id: str):
    """
    Background task: Create paper trading position
    """
    simulator = PositionSimulator()
    match = db.get_match(match_id)
    position = simulator.create_position(match)
    
    db.insert_position(position)

# Scheduled tasks
@celery.task
def update_all_prices():
    """
    Periodic task: Update market prices for open positions
    Run every hour
    """
    simulator = PositionSimulator()
    simulator.update_prices()

@celery.task
def check_resolutions():
    """
    Periodic task: Check for market resolutions
    Run every 10 minutes
    """
    monitor = ResolutionMonitor()
    monitor.run()

# Celery beat schedule
celery.conf.beat_schedule = {
    'update-prices-hourly': {
        'task': 'update_all_prices',
        'schedule': 3600.0,  # Every hour
    },
    'check-resolutions': {
        'task': 'check_resolutions',
        'schedule': 600.0,  # Every 10 minutes
    },
}
```

---

## 11. MONITORING & OBSERVABILITY

### 11.1 Logging

**Structured Logging:**
```python
import structlog
import logging

# Configure structlog
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Usage
logger.info("prediction_extracted", 
            prediction_id=prediction.id,
            pundit_id=prediction.pundit_id,
            claim=prediction.claim[:50])

logger.error("market_match_failed",
             prediction_id=prediction.id,
             error=str(e),
             exc_info=True)
```

### 11.2 Metrics

**Prometheus Metrics:**
```python
from prometheus_client import Counter, Histogram, Gauge

# Counters
predictions_extracted = Counter('predictions_extracted_total', 
                                'Total predictions extracted')
matches_found = Counter('matches_found_total', 
                       'Total market matches found')
positions_created = Counter('positions_created_total', 
                           'Total positions created')

# Histograms
extraction_duration = Histogram('extraction_duration_seconds', 
                               'Time to extract predictions')
matching_duration = Histogram('matching_duration_seconds', 
                             'Time to match prediction to market')

# Gauges
open_positions = Gauge('open_positions', 
                      'Number of open positions')
pending_matches = Gauge('pending_matches', 
                       'Predictions awaiting match')

# Usage
with extraction_duration.time():
    predictions = extractor.extract(raw_content)
    predictions_extracted.inc(len(predictions))
```

**Custom Metrics Dashboard:**
```python
@app.get("/api/metrics")
async def get_metrics(db = Depends(get_db)):
    """
    System health metrics
    """
    return {
        "database": {
            "predictions_total": await db.fetchval("SELECT COUNT(*) FROM predictions"),
            "predictions_today": await db.fetchval("""
                SELECT COUNT(*) FROM predictions 
                WHERE captured_at > NOW() - INTERVAL '24 hours'
            """),
            "open_positions": await db.fetchval("""
                SELECT COUNT(*) FROM positions WHERE status = 'open'
            """),
            "pending_matches": await db.fetchval("""
                SELECT COUNT(*) FROM predictions WHERE status = 'pending_match'
            """)
        },
        "performance": {
            "avg_extraction_time": extraction_duration.observe(),
            "avg_matching_time": matching_duration.observe()
        },
        "queues": {
            "raw_content": await redis.llen('raw_content'),
            "predictions": await redis.llen('predictions'),
            "matches": await redis.llen('matches')
        }
    }
```

### 11.3 Alerting

**CloudWatch Alarms:**
```python
import boto3

cloudwatch = boto3.client('cloudwatch')

# Alert on high error rate
cloudwatch.put_metric_alarm(
    AlarmName='TrackRecord-HighErrorRate',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=2,
    MetricName='Errors',
    Namespace='TrackRecord/API',
    Period=300,
    Statistic='Sum',
    Threshold=10,
    ActionsEnabled=True,
    AlarmActions=[
        'arn:aws:sns:us-east-1:123456789:trackrecord-alerts'
    ],
    AlarmDescription='Alert when error rate exceeds threshold'
)

# Alert on database connection pool exhaustion
cloudwatch.put_metric_alarm(
    AlarmName='TrackRecord-DBPoolExhausted',
    ComparisonOperator='GreaterThanThreshold',
    EvaluationPeriods=1,
    MetricName='DatabaseConnections',
    Namespace='TrackRecord/Database',
    Period=60,
    Statistic='Maximum',
    Threshold=45,  # Out of 50 max
    ActionsEnabled=True,
    AlarmActions=[
        'arn:aws:sns:us-east-1:123456789:trackrecord-alerts'
    ]
)
```

---

## 12. DEVELOPMENT WORKFLOW

### 12.1 Local Development Setup
```bash
# Clone repo
git clone https://github.com/trackrecord/trackrecord.git
cd trackrecord

# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Set up environment variables
cp .env.example .env
# Edit .env with your credentials

# Start services
docker-compose up -d postgres redis

# Run migrations
alembic upgrade head

# Start API
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Start Celery worker (in another terminal)
celery -A tasks worker --loglevel=info

# Start Celery beat (in another terminal)
celery -A tasks beat --loglevel=info
```

### 12.2 Testing
```python
# tests/test_prediction_extraction.py

import pytest
from app.services.prediction_extractor import PredictionExtractor

@pytest.mark.asyncio
async def test_extract_prediction_from_tweet():
    extractor = PredictionExtractor()
    
    raw_content = RawContent(
        text="Trump will definitely win Pennsylvania. Mark my words!",
        author="Test Pundit",
        timestamp=datetime(2024, 1, 1),
        source_url="https://twitter.com/test/status/123"
    )
    
    predictions = await extractor.extract(raw_content)
    
    assert len(predictions) == 1
    assert "Trump" in predictions[0].claim
    assert "Pennsylvania" in predictions[0].claim
    assert predictions[0].confidence > 0.8  # "definitely"

@pytest.mark.asyncio
async def test_no_predictions_in_opinion():
    extractor = PredictionExtractor()
    
    raw_content = RawContent(
        text="I think the economy is doing poorly right now.",
        author="Test Pundit",
        timestamp=datetime(2024, 1, 1),
        source_url="https://twitter.com/test/status/124"
    )
    
    predictions = await extractor.extract(raw_content)
    
    assert len(predictions) == 0  # Opinion, not prediction

# Run tests
# pytest tests/ -v --cov=app
```

### 12.3 Code Quality
```bash
# .pre-commit-config.yaml

repos:
  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/PyCQA/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args: ['--max-line-length=100']

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.3.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]

# Install pre-commit hooks
pre-commit install
```

---

## SUMMARY

This is the **complete technical architecture** for TrackRecord. We've covered:

1. ✅ **System Overview** - High-level responsibilities
2. ✅ **Architecture** - Multi-layer design with separation of concerns
3. ✅ **Data Pipeline** - Ingestion from Twitter, podcasts, news
4. ✅ **Core Services** - NLP extraction, market matching, position simulation
5. ✅ **Database** - PostgreSQL schema with immutability & performance
6. ✅ **API** - RESTful endpoints with caching & rate limiting
7. ✅ **Frontend** - Next.js with React Query
8. ✅ **Infrastructure** - AWS (EC2, RDS, ElastiCache) + Terraform
9. ✅ **Security** - Encryption, auth, CORS, SQL injection prevention
10. ✅ **Scalability** - Multi-layer caching, connection pooling, async jobs
11. ✅ **Monitoring** - Structured logging, Prometheus metrics, CloudWatch
12. ✅ **DevOps** - Docker, CI/CD, testing

**This is production-ready architecture.** Every component is designed to scale, be monitored, and fail gracefully.

**What would you like to dive deeper into, boss? Or should we start building?**
